import React, { useState, useEffect, useCallback, useRef } from 'react';
import { AutoTokenizer } from '@xenova/transformers';
import { LLM } from './llm';
import { marked } from 'marked';
import PdfParser from './PdfParser';
import './App.css';

const preCannedQueries = {
  "1": "Tell me about the lighthouse of Alexandria.",
  "2": "Did the lighthouse of Alexandria existed at the same time the library of Alexandria existed?",
  "3": "How did the Pharos lighthouse impact ancient maritime trade?",
  "4": "Tell me about Constantinople.",
};

marked.use({ mangle: false, headerIds: false });

const BotB = ({ pdfContent, llmB, tokenizer, setChatHistory }) => {
  const CHUNK_SIZE = 1000;

  const splitIntoChunks = (text, size) => {
    const regex = new RegExp(`(.|[\r\n]){1,${size}}`, 'g');
    const chunks = text.match(regex) || [];
    return chunks;
  };

  const normalizeText = (text) => {
    return text.toLowerCase().replace(/[\W_]+/g, ' ').trim();
  };

  const searchPDF = async (query) => {
    setChatHistory((prevHistory) => [
      ...prevHistory,
      { type: 'bot', text: "Analyzing PDF, please wait ...", botType: 'botB' },
    ]);

    const chunks = splitIntoChunks(pdfContent, CHUNK_SIZE);
    const results = [];

    for (const chunk of chunks) {
      const normalizedChunk = normalizeText(chunk);
     const prompt = `<|system|>\nYou are an assistant. You are given some content of a pdf/file as following, answer the question/query based on the content.<|end|>\n <|user|>\n ${normalizedChunk}.${query}<|end|>\n<|assistant|>`;
      const promptText = 'You are an assistant. You are given some content of a pdf/file as following, answer the question/query based on the content.';
  
      const { input_ids } = await tokenizer(prompt, { return_tensor: false, padding: true, truncation: true });

      await llmB.initialize_feed();
      try {
        const output_tokens = await llmB.generate(input_ids, null, { max_tokens: 9999 });
        const responseText = tokenizer.decode(output_tokens, { skip_special_tokens: true });

        const promptTextLength = promptText.length;
        const normalizedChunkTextLength = normalizedChunk.length;
        const queryTextLength = query.length;
        const assistantresponse = responseText.substring(promptTextLength + normalizedChunkTextLength + queryTextLength + 1);

        setChatHistory((prevHistory) => [
          ...prevHistory,
          { type: 'bot', text: marked.parse(assistantresponse.trim()), botType: 'botB' },
        ]);

        results.push(assistantresponse.trim());
      } catch (error) {
        console.error(`Error in beta.generate: ${error.message}`);
        throw error;
      }
    }
    return results;
  };
  return { searchPDF };
};

const BotC = ({ llmC, tokenizer }) => {

  const respond = async (message) => {
    let prompt = `<|system|>\nYou are a friendly assistant.<|end|>\n<|user|>\n${message}<|end|>\n<|assistant|>`;

    const { input_ids } = await tokenizer(prompt, { return_tensor: false, padding: true, truncation: true });

    await llmC.initialize_feed();

    const start_timer = performance.now();
    const output_index = llmC.output_tokens.length + input_ids.length;

    try {
      const output_tokens = await llmC.generate(input_ids, (output_tokens) => {
        if (output_tokens.length === input_ids.length + 1) {
          const took = (performance.now() - start_timer) / 1000;
          console.log(`time to first token in ${took.toFixed(1)}sec, ${input_ids.length} tokens`);
        }
      }, { max_tokens: 9999 });

      const took = (performance.now() - start_timer) / 1000;
      const responseText = tokenizer.decode(output_tokens.slice(output_index), { skip_special_tokens: true });
      const seqlen = output_tokens.length - output_index;
      console.log(`${seqlen} tokens in ${took.toFixed(1)}sec, ${(seqlen / took).toFixed(2)} tokens/sec`);
      llmC.clear_buffers();
      return responseText;
    } catch (error) {
      console.error(`Error in LLM.generate: ${error.message}`);
      throw error;
    }
  };

  return { respond };
};

const BotA = ({ pdfContent, botB, botC, llmA, tokenizer, setChatHistory }) => {

  const respond = async (message) => {
    const isPDFRelated = await analyzeQuery(message);

    if (isPDFRelated) {
      if (!pdfContent) {
        return { text: "Please upload a PDF document!", botType: 'botA' };
      }
      const intermediateResults = await botB.searchPDF(message);
      const finalResult = await generateResponse(intermediateResults.join('\n') + message + ". In short!");
      return { text: "Confirmed: \n" + finalResult, botType: 'botA' };
    } else {
      const response = await botC.respond(message);
      return { text: response, botType: 'botC' };
    }
  };

  const analyzeQuery = async (message) => {
   let prompt =  `<|system|>\nYou are a friendly assistant. Does the message ask about a pdf document or something in the file? Answer yes or no.<|end|>\n<|user|>\n${message}<|end|>\n<|assistant|>\n`;

   
    const { input_ids } = await tokenizer(prompt, { return_tensor: false, padding: true, truncation: true });

    await llmA.initialize_feed();

    const start_timer = performance.now();
    const output_index = llmA.output_tokens.length + input_ids.length;

    try {
      const output_tokens = await llmA.generate(input_ids, (output_tokens) => {
        if (output_tokens.length === input_ids.length + 1) {
          const took = (performance.now() - start_timer) / 1000;
          console.log(`time to first token in ${took.toFixed(1)}sec, ${input_ids.length} tokens`);
        }
      }, { max_tokens: 9999 });

      const took = (performance.now() - start_timer) / 1000;
      const responseText = tokenizer.decode(output_tokens.slice(output_index), { skip_special_tokens: true });
      const seqlen = output_tokens.length - output_index;
      console.log(`${seqlen} tokens in ${took.toFixed(1)}sec, ${(seqlen / took).toFixed(2)} tokens/sec`);

      console.log("alpha:", responseText);
      setChatHistory((prevHistory) => [
        ...prevHistory,
        { type: 'bot', text: marked.parse(responseText.trim()), botType: 'botA' },
      ]);

      return responseText.toLowerCase().startsWith('yes');
    } catch (error) {
      console.error(`Error in alpha.generate: ${error.message}`);
      throw error;
    }
  };

  const generateResponse = async (message) => {
    let prompt = `\nYou are a friendly assistant.\n\n${message}\n`;

    const { input_ids } = await tokenizer(prompt, { return_tensor: false, padding: true, truncation: true });

    await llmA.initialize_feed();

    const start_timer = performance.now();
    const output_index = llmA.output_tokens.length + input_ids.length;

    try {
      const output_tokens = await llmA.generate(input_ids, (output_tokens) => {
        if (output_tokens.length === input_ids.length + 1) {
          const took = (performance.now() - start_timer) / 1000;
          console.log(`time to first token in ${took.toFixed(1)}sec, ${input_ids.length} tokens`);
        }
      }, { max_tokens: 9999 });

      const took = (performance.now() - start_timer) / 1000;
      const responseText = tokenizer.decode(output_tokens.slice(output_index), { skip_special_tokens: true });
      const seqlen = output_tokens.length - output_index;
      console.log(`${seqlen} tokens in ${took.toFixed(1)}sec, ${(seqlen / took).toFixed(2)} tokens/sec`);
      return responseText;
    } catch (error) {
      console.error(`Error in LLM.generate: ${error.message}`);
      throw error;
    }
  };

  return { respond };
};

const App = () => {
  const [input, setInput] = useState('');
  const [chatHistory, setChatHistory] = useState([]);
  const [isSending, setIsSending] = useState(false);
  const [llmA, setLLMA] = useState(null);
  const [llmB, setLLMB] = useState(null);
  const [llmC, setLLMC] = useState(null);
  const [tokenizer, setTokenizer] = useState(null);
  const [status, setStatus] = useState('');
  const [pdfContent, setPdfContent] = useState('');
  const chatContainerRef = useRef(null);

  const config = {
    model: "phi3",
    provider: "webgpu",
    profiler: 0,
    verbose: 0,
    threads: 1,
    show_special: 0,
    csv: 0,
    max_tokens: 9999,
    local: 1,
  };

  const log = (message) => {
    setStatus((prevStatus) => `${prevStatus}\n${message}`);
    setTimeout(() => {
      setStatus('');
    }, 3000);
  };

  const Init = useCallback(async (llmInstance, hasFP16) => {
    try {
      log("Loading model...");
      await llmInstance.load(config.model, {
        provider: config.provider,
        profiler: config.profiler,
        verbose: config.verbose,
        local: config.local,
        max_tokens: config.max_tokens,
        hasFP16: hasFP16 === 2,
      });
      log("Ready.");
      if (hasFP16 === 2) {
        console.log("WebGPU supports fp16.");
      } else if (hasFP16 === 1) {
        console.log("WebGPU does not support fp16, using fp32 instead.");
      } else {
        console.log("WebGPU is not supported.");
      }
    } catch (error) {
      log(`Model loading failed: ${error.message}`);
    }
  }, []);

  useEffect(() => {
    async function initialize() {
      try {
        const llmInstanceA = new LLM();
        const llmInstanceB = new LLM();
        const llmInstanceC = new LLM();

        setLLMA(llmInstanceA);
        setLLMB(llmInstanceB);
        setLLMC(llmInstanceC);

        log("Loading tokenizer...");
        const tokenizerInstance = await AutoTokenizer.from_pretrained('');
        setTokenizer(() => tokenizerInstance);
        log("Tokenizer loaded successfully.");

        const hasFP16 = await checkWebGPU();
        await Init(llmInstanceA, hasFP16);
        await Init(llmInstanceB, hasFP16);
        await Init(llmInstanceC, hasFP16);

      } catch (error) {
        console.error("Failed to initialize:", error);
        log(`Failed to initialize: ${error.message}`);
      }
    }
    initialize();
  }, [Init]);

  const checkWebGPU = async () => {
    if (!("gpu" in navigator)) {
      return 0;
    }
    try {
      const adapter = await navigator.gpu.requestAdapter();
      if (adapter.features.has('shader-f16')) {
        return 2;
      }
      return 1;
    } catch (e) {
      return 0;
    }
  };

  const submitRequest = async (e) => {
    e.preventDefault();

    if (isSending) {
      llmA.abort();
      llmB.abort();
      llmC.abort();
      setIsSending(false);
      return;
    }

    if (!tokenizer) {
      log("Tokenizer not initialized.");
      return;
    }

    const userMessage = input;
    if (!userMessage.trim()) return;

    setChatHistory((prevHistory) => [...prevHistory, { type: 'user', text: userMessage }]);
    setInput('');
    setIsSending(true);

    try {
      const botB = BotB({ pdfContent, llmB, tokenizer, setChatHistory });
      const botC = BotC({ llmC, tokenizer, setChatHistory });
      const botA = BotA({ pdfContent, botB, botC, llmA, tokenizer, setChatHistory });
      const response = await botA.respond(userMessage);
      setChatHistory((prevHistory) => [
        ...prevHistory,
        { type: 'bot', text: marked.parse(response.text), botType: response.botType },
      ]);
    } catch (error) {
      console.error(error);
      setChatHistory((prevHistory) => [
        ...prevHistory,
        { type: 'error', text: `Error generating response: ${error.message}` },
      ]);
    } finally {
      setIsSending(false);
    }
  };

  useEffect(() => {
    if (chatContainerRef.current) {
      chatContainerRef.current.scrollTop = chatContainerRef.current.scrollHeight;
    }
  }, [chatHistory]);

  const handleKeyDown = (e) => {
    if (e.ctrlKey) {
      if (e.key === 'Enter') {
        submitRequest(e);
      } else {
        const query = preCannedQueries[e.key];
        if (query) {
          setInput(query);
          submitRequest(e);
        }
      }
    } else if (e.key === 'Enter') {
      e.preventDefault();
      submitRequest(e);
    }
  };

  const handlePdfParsed = (text) => {
    setPdfContent(text);
    setChatHistory((prevHistory) => [
      ...prevHistory,
      { type: 'bot', text: marked.parse(`PDF parsed successfully!`), botType: 'botB' },
    ]);
  };

  return (
    <div className="container">
      <div className="row pt-3">
        <div className="col-md-8 col-12">
          <h2>My ChatBot ONNX</h2>
        </div>
        <div id="status" className="col-md-12 col-12">{status}</div>
      </div>
      <div id="chat-container" ref={chatContainerRef}>
        <div id="chat-history">
          {chatHistory.map((message, index) => (
            <div key={index} className={message.type === 'user' ? 'user-message mb-2' : `response-message mb-2 text-start ${message.botType}`}>
              <span dangerouslySetInnerHTML={{ __html: message.text }} />
              {message.botType && <div className="bot-label">{message.botType === 'botA' ? 'Alpha' : message.botType === 'botB' ? 'Beta' : 'Gamma'}</div>}
            </div>
          ))}
        </div>
      </div>
      <div id="input-area">
        <textarea
          id="user-input"
          placeholder="Type your question here ..."
          value={input}
          onChange={(e) => setInput(e.target.value)}
          onKeyDown={handleKeyDown}
        ></textarea>
        <button id="send-button" onClick={submitRequest}>
          {isSending ? 'Stop' : 'Send'}
        </button>
        <PdfParser onPdfParsed={handlePdfParsed} />
      </div>
    </div>
  );
};

export default App;
